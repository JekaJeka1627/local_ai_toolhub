# Local AI Toolhub

A local AI assistant with RAG capabilities and tool integration.

## Features

- ðŸ¤– Multiple AI model support (Qwen3, Holo1)
- ðŸ“š RAG (Retrieval-Augmented Generation) for book queries
- ðŸ”§ MCP (Model Context Protocol) tool integration
- ðŸ’¬ Streamlit web interface

## Setup

1. **Install Dependencies**
   ```bash
   pip install streamlit llama-index chromadb huggingface-hub
   ```

2. **Configure Hugging Face (Optional)**
   ```bash
   huggingface-cli login
   ```

3. **Run the Application**
   ```bash
   streamlit run local_chat.py
   ```

## Project Structure

```
local_ai_toolhub/
â”œâ”€â”€ local_chat.py          # Main Streamlit interface
â”œâ”€â”€ model_clients.py       # AI model client interfaces
â”œâ”€â”€ tools.py              # MCP tools and utilities
â”œâ”€â”€ rag/
â”‚   â””â”€â”€ holo_rag.py       # RAG implementation
â”œâ”€â”€ rag/chroma_store/     # Vector database (auto-created)
â””â”€â”€ Books/                # Document collection
```

## Usage

1. Open http://localhost:8501 in your browser
2. Select a model (Qwen3 for general queries, Holo1 for book RAG)
3. Enter your prompt and optionally select tools
4. Click "Run" to get responses

## Requirements

- Python 3.8+
- LM Studio running locally (for Qwen3 model)
- Book collection in the specified directory